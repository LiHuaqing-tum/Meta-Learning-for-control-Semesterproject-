{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f32253c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f32253c",
        "outputId": "90baf83a-c9f6-4243-acfb-2ae106961e72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m1.7/1.8 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'Meta-Learning-for-control-Semesterproject-'...\n",
            "remote: Enumerating objects: 118, done.\u001b[K\n",
            "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 118 (delta 15), reused 103 (delta 9), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (118/118), 4.81 MiB | 9.71 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "/content/Meta-Learning-for-control-Semesterproject-\n"
          ]
        }
      ],
      "source": [
        "%pip install -q --upgrade pip\n",
        "%pip install -q \"jax[cuda12]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "%pip install -q optax matplotlib\n",
        "%pip install -q mujoco-mjx\n",
        "%pip install -q stljax\n",
        "import os\n",
        "\n",
        "repo_url = \"https://github.com/LiHuaqing-tum/Meta-Learning-for-control-Semesterproject-.git\"\n",
        "repo_name = \"Meta-Learning-for-control-Semesterproject-\"\n",
        "\n",
        "if not os.path.exists(repo_name):\n",
        "    !git clone {repo_url}\n",
        "else:\n",
        "    %cd {repo_name}\n",
        "    !git pull origin main\n",
        "    %cd ..\n",
        "\n",
        "%cd {repo_name}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax, mujoco, optax\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeK258jS06c1",
        "outputId": "8952a6ab-6303-4bc1-f3c2-2cd656710c00"
      },
      "id": "oeK258jS06c1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Setup complete!\n",
            "JAX version: 0.5.3\n",
            "MuJoCo version: 3.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb3c3bb7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb3c3bb7",
        "outputId": "924640f3-ba92-461d-f722-5b02d5d70f22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to import warp: No module named 'warp'\n",
            "Failed to import mujoco.mjx.third_party.mujoco_warp as mujoco_warp: No module named 'warp'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import pathlib\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from mujoco import mjx\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from stljax.formula import *\n",
        "from stljax.viz import *\n",
        "import optax\n",
        "import functools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "xml_path  = repo_root / \"task\" / \"scenes\" / \"panda_push_scene_without_obstacle.xml\"\n",
        "from src.environment.SImpleReacherEnv import SimpleReacherEnv\n"
      ],
      "metadata": {
        "id": "caOECsV0rRyv"
      },
      "id": "caOECsV0rRyv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f7d60d9",
      "metadata": {
        "id": "7f7d60d9"
      },
      "outputs": [],
      "source": [
        "def sdist_circle_2d(p_xy, center_xy, radius):\n",
        "\n",
        "    return jnp.linalg.norm(p_xy - center_xy, axis=-1) - radius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a9de4f",
      "metadata": {
        "id": "19a9de4f"
      },
      "outputs": [],
      "source": [
        "\n",
        "GREEN_C = jnp.array([0.0, 0.90])\n",
        "GREEN_H = jnp.array([0.08, 0.08])\n",
        "R_OBS   = jnp.linalg.norm(GREEN_H)\n",
        "\n",
        "RED_C   = jnp.array([0.0, 1.40])\n",
        "RED_H   = jnp.array([0.15, 0.15])\n",
        "R_GOAL  = jnp.minimum(RED_H[0], RED_H[1])\n",
        "\n",
        "BLOCK_H = jnp.array([0.06, 0.06])\n",
        "R_BLOCK = jnp.linalg.norm(BLOCK_H)\n",
        "\n",
        "\n",
        "def avoid_green_signal_circ_circ(states_xy, obstacle_center_xy, safe_margin=0.02):\n",
        "    req = R_BLOCK + R_OBS + safe_margin\n",
        "    return jnp.linalg.norm(states_xy - obstacle_center_xy, axis=-1) - req\n",
        "\n",
        "\n",
        "def reach_red_signal_circ_goal(states_xy, target_center_xy, inside_tol=0.02):\n",
        "    req_in = jnp.maximum(R_GOAL - (R_BLOCK + inside_tol), 0.0)\n",
        "    return req_in - jnp.linalg.norm(states_xy - target_center_xy, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e05baaf",
      "metadata": {
        "id": "0e05baaf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "env = SimpleReacherEnv(\n",
        "    model_path=str(repo_root / \"task\" / \"scenes\" / \"panda_push_scene_without_obstacle.xml\"),\n",
        "    return_full_qpos_qvel=False,\n",
        ")\n",
        "_ = env.reset(jax.random.PRNGKey(0))\n",
        "initial_data = env.d\n",
        "\n",
        "\n",
        "obstacle_center_xy = env.obstacle_center_xy\n",
        "target_center_xy = env.target_center_xy\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_spec(obstacle_center_xy, target_center_xy, safe_margin=0.02, inside_tol=0.02):\n",
        "    avoid_pred = Predicate(\n",
        "        \"avoid_green_margin\",\n",
        "        lambda traj_xy: avoid_green_signal_circ_circ(traj_xy, obstacle_center_xy, safe_margin)\n",
        "    )\n",
        "    reach_pred = Predicate(\n",
        "        \"reach_red_margin\",\n",
        "        lambda traj_xy: reach_red_signal_circ_goal(traj_xy, target_center_xy, inside_tol)\n",
        "    )\n",
        "    avoid_atom = avoid_pred > 0.0\n",
        "    reach_atom = reach_pred > 0.0\n",
        "    return avoid_atom, reach_atom\n",
        "\n",
        "avoid_atom, reach_atom = build_spec(obstacle_center_xy, target_center_xy)\n",
        "horizon=50\n",
        "spec = Always(avoid_atom, interval=(0, horizon)) & Eventually(reach_atom, interval=(0, horizon))"
      ],
      "metadata": {
        "id": "oehzdIJTAi_v"
      },
      "id": "oehzdIJTAi_v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.ctrlrange_jnp[:, 0])\n",
        "print(env.ctrlrange_jnp[:, 1])\n",
        "print(\"tolerance=\", float(env.m.opt.tolerance),\n",
        "      \"iterations=\", int(env.m.opt.iterations))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKF0fJnufZGw",
        "outputId": "7f74d989-0ef7-4233-d433-7437a43e6456"
      },
      "id": "NKF0fJnufZGw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-87. -87. -87. -87. -12. -12. -12.]\n",
            "[87. 87. 87. 87. 12. 12. 12.]\n",
            "tolerance= 0.0 iterations= 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec5e7db4",
      "metadata": {
        "id": "ec5e7db4"
      },
      "outputs": [],
      "source": [
        "\n",
        "obs_dim = env.get_observation_space()[0]\n",
        "act_dim = env.get_action_space()[0]\n",
        "\n",
        "\n",
        "layer_sizes = [obs_dim, 64, 64, act_dim]\n",
        "\n",
        "def init_layer_params(key, in_dim, out_dim):\n",
        "    w_key, b_key = jax.random.split(key)\n",
        "    # Enhanced numerical stability: prevent division by zero and use smaller initial weights\n",
        "    fan_in_out = jnp.maximum(in_dim + out_dim, 1.0)  # Prevent division by zero\n",
        "    glorot = jnp.sqrt(2.0 / fan_in_out) * 0.1  # Scale down initial weights for stability\n",
        "    W = glorot * jax.random.normal(w_key, (out_dim, in_dim))\n",
        "    b = jnp.zeros((out_dim,))\n",
        "    return W, b\n",
        "def init_policy_params(key, sizes):\n",
        "    keys = jax.random.split(key, len(sizes) - 1)\n",
        "    return [init_layer_params(k, sizes[i], sizes[i + 1]) for i, k in enumerate(keys)]\n",
        "\n",
        "policy_key = jax.random.PRNGKey(42)\n",
        "policy_params = init_policy_params(policy_key, layer_sizes)\n",
        "\n",
        "\n",
        "tau_lo = env.ctrlrange_jnp[:, 0] if env.ctrlrange_jnp.size else None\n",
        "tau_hi = env.ctrlrange_jnp[:, 1] if env.ctrlrange_jnp.size else None\n",
        "\n",
        "def policy_apply(params, obs):\n",
        "    x = obs\n",
        "    for W, b in params[:-1]:\n",
        "        x = jnp.tanh(W @ x + b)\n",
        "    W, b = params[-1]\n",
        "    raw = jnp.tanh(W @ x + b)\n",
        "    if tau_lo is not None:\n",
        "        raw_tau = tau_lo + 0.5 * (raw + 1.0) * (tau_hi - tau_lo)\n",
        "        return 0.2 * raw_tau\n",
        "    return raw\n",
        "\n",
        "\n",
        "def block_xy_from_data(data: mjx.Data) -> jnp.ndarray:\n",
        "    return data.site_xpos[env.site_id_block, :2]\n",
        "\n",
        "def rollout_block_xy(params, data0):\n",
        "    obs0 = env._obs_from_data(data0)\n",
        "    xy0 = block_xy_from_data(data0)\n",
        "\n",
        "    def body_fn(carry, _):\n",
        "        data, obs = carry\n",
        "        act = policy_apply(params, obs)\n",
        "        data_new = data.replace(ctrl=act)\n",
        "        data_new = mjx.step(env.m, data_new)\n",
        "        obs_new = env._obs_from_data(data_new)\n",
        "        xy_new = block_xy_from_data(data_new)\n",
        "        return (data_new, obs_new), (obs_new, act, xy_new)\n",
        "\n",
        "    (_, _), (obs_seq, act_seq, xy_seq) = jax.lax.scan(\n",
        "        body_fn,\n",
        "        (data0, obs0),\n",
        "        jnp.arange(horizon)\n",
        "    )\n",
        "\n",
        "    obs_traj = jnp.vstack([obs0, obs_seq])            # (horizon+1, obs_dim)\n",
        "    xy_traj = jnp.vstack([xy0, xy_seq])               # (horizon+1, 2)\n",
        "    return xy_traj, obs_traj, act_seq\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_weights = jnp.array([1.0, 0.05, 0.05])\n",
        "torque_limit = jnp.max(jnp.abs(env.ctrlrange_jnp))\n",
        "\n",
        "def loss_fn(params):\n",
        "    traj_xy, _, act_seq = rollout_block_xy(params, initial_data)\n",
        "\n",
        "\n",
        "    traj_xy = jnp.nan_to_num(traj_xy, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "    act_seq = jnp.nan_to_num(act_seq, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "    rho = jnp.asarray(spec.robustness(traj_xy))\n",
        "    robustness = rho.reshape(-1)[-1]\n",
        "\n",
        "\n",
        "    robustness = jnp.nan_to_num(robustness, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "    loss_robustness = jax.nn.relu(-robustness)\n",
        "\n",
        "\n",
        "    diffs = jnp.diff(act_seq, axis=0)\n",
        "    diffs = jnp.clip(diffs, -10.0, 10.0)\n",
        "    total_variation = jnp.mean(jnp.sum(jnp.abs(diffs), axis=1))\n",
        "\n",
        "\n",
        "    act_seq_clipped = jnp.clip(act_seq, -5.0, 5.0)\n",
        "    l2_energy = jnp.mean(jnp.sum(act_seq_clipped**2, axis=1))\n",
        "    loss_smooth = total_variation + l2_energy\n",
        "\n",
        "\n",
        "    tau_norm = jnp.linalg.norm(act_seq, axis=1)\n",
        "    tau_norm = jnp.clip(tau_norm, 0.0, 1e6)\n",
        "    loss_limits = jnp.mean(jax.nn.relu(tau_norm - torque_limit))\n",
        "\n",
        "    cost_vec = jnp.array([\n",
        "        loss_robustness,\n",
        "        loss_smooth,\n",
        "        loss_limits,\n",
        "    ])\n",
        "\n",
        "\n",
        "    cost_vec = jnp.nan_to_num(cost_vec, nan=1e6, posinf=1e6, neginf=-1e6)\n",
        "    final_loss = jnp.dot(loss_weights, cost_vec)\n",
        "\n",
        "    return jnp.nan_to_num(final_loss, nan=1e6, posinf=1e6, neginf=-1e6)"
      ],
      "metadata": {
        "id": "_yrgVRCfu2sg"
      },
      "id": "_yrgVRCfu2sg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import jacfwd\n",
        "from jax.flatten_util import ravel_pytree\n",
        "import optax"
      ],
      "metadata": {
        "id": "2ISgF4_Wg-I3"
      },
      "id": "2ISgF4_Wg-I3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optax\n",
        "from jax.flatten_util import ravel_pytree\n",
        "\n",
        "\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(1.0),\n",
        "    optax.adam(5e-4)\n",
        ")\n",
        "opt_state = optimizer.init(policy_params)\n",
        "\n",
        "flat0, unravel = ravel_pytree(policy_params)\n",
        "\n",
        "def loss_flat(p_flat):\n",
        "    return loss_fn(unravel(p_flat))\n",
        "\n",
        "@jax.jit\n",
        "def forward_mode_grad(p_flat, chunk=1024):\n",
        "    n = p_flat.size\n",
        "    g = jnp.zeros_like(p_flat)\n",
        "\n",
        "\n",
        "    p_flat = jnp.nan_to_num(p_flat, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "    eye = jnp.eye(n, dtype=p_flat.dtype)\n",
        "    for start in range(0, n, chunk):\n",
        "        stop = min(start + chunk, n)\n",
        "        E = eye[start:stop]\n",
        "\n",
        "        def jvp_dir(v):\n",
        "            _, jvp_val = jax.jvp(loss_flat, (p_flat,), (v,))\n",
        "            return jnp.nan_to_num(jvp_val, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "        block = jax.vmap(jvp_dir)(E)  # (k,)\n",
        "        g = g.at[start:stop].set(block)\n",
        "    return g\n",
        "\n",
        "@jax.jit\n",
        "def update(params, opt_state):\n",
        "    p_flat, unravel = ravel_pytree(params)\n",
        "\n",
        "\n",
        "    p_flat = jnp.nan_to_num(p_flat, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "    loss_val = loss_flat(p_flat)\n",
        "    g_flat = forward_mode_grad(p_flat)\n",
        "\n",
        "\n",
        "    g_flat = jnp.clip(g_flat, -0.5, 0.5)\n",
        "    g_flat = jnp.nan_to_num(g_flat, nan=0.0, posinf=0.5, neginf=-0.5)\n",
        "\n",
        "    grads = unravel(g_flat)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "\n",
        "    params = jax.tree_map(lambda x: jnp.nan_to_num(x, nan=0.0, posinf=1e6, neginf=-1e6), params)\n",
        "\n",
        "    return params, opt_state, loss_val"
      ],
      "metadata": {
        "id": "noO_7R-eu9FC"
      },
      "id": "noO_7R-eu9FC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80a00c9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80a00c9e",
        "outputId": "71e00870-744a-4ff1-92b2-0905af07fe6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4234164737.py:56: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  params = jax.tree_map(lambda x: jnp.nan_to_num(x, nan=0.0, posinf=1e6, neginf=-1e6), params)\n",
            "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/xla.py:132: RuntimeWarning: overflow encountered in cast\n",
            "  return np.asarray(x, dtypes.canonicalize_dtype(x.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0001 | STL loss -0.2180 robustness\n",
            "step 0002 | STL loss -0.2180 robustness\n",
            "step 0003 | STL loss -0.2180 robustness\n",
            "step 0004 | STL loss -0.2180 robustness\n",
            "step 0005 | STL loss -0.2180 robustness\n",
            "step 0006 | STL loss -0.2180 robustness\n",
            "step 0007 | STL loss -0.2180 robustness\n",
            "step 0008 | STL loss -0.2180 robustness\n",
            "step 0009 | STL loss -0.2180 robustness\n",
            "step 0010 | STL loss -0.2180 robustness\n",
            "step 0011 | STL loss -0.2180 robustness\n",
            "step 0012 | STL loss -0.2180 robustness\n",
            "step 0013 | STL loss -0.2180 robustness\n",
            "step 0014 | STL loss -0.2180 robustness\n",
            "step 0015 | STL loss -0.2180 robustness\n",
            "step 0016 | STL loss -0.2180 robustness\n",
            "step 0017 | STL loss -0.2180 robustness\n",
            "step 0018 | STL loss -0.2180 robustness\n",
            "step 0019 | STL loss -0.2180 robustness\n",
            "step 0020 | STL loss -0.2180 robustness\n",
            "step 0021 | STL loss -0.2180 robustness\n",
            "step 0022 | STL loss -0.2180 robustness\n",
            "step 0023 | STL loss -0.2180 robustness\n",
            "step 0024 | STL loss -0.2180 robustness\n",
            "step 0025 | STL loss -0.2180 robustness\n",
            "step 0026 | STL loss -0.2180 robustness\n",
            "step 0027 | STL loss -0.2180 robustness\n",
            "step 0028 | STL loss -0.2180 robustness\n",
            "step 0029 | STL loss -0.2180 robustness\n",
            "step 0030 | STL loss -0.2180 robustness\n",
            "step 0031 | STL loss -0.2180 robustness\n",
            "step 0032 | STL loss -0.2180 robustness\n",
            "step 0033 | STL loss -0.2180 robustness\n",
            "step 0034 | STL loss -0.2180 robustness\n",
            "step 0035 | STL loss -0.2180 robustness\n",
            "step 0036 | STL loss -0.2180 robustness\n",
            "step 0037 | STL loss -0.2180 robustness\n",
            "step 0038 | STL loss -0.2180 robustness\n",
            "step 0039 | STL loss -0.2180 robustness\n",
            "step 0040 | STL loss -0.2180 robustness\n",
            "step 0041 | STL loss -0.2180 robustness\n",
            "step 0042 | STL loss -0.2180 robustness\n",
            "step 0043 | STL loss -0.2180 robustness\n",
            "step 0044 | STL loss -0.2180 robustness\n",
            "step 0045 | STL loss -0.2180 robustness\n",
            "step 0046 | STL loss -0.2180 robustness\n",
            "step 0047 | STL loss -0.2180 robustness\n",
            "step 0048 | STL loss -0.2180 robustness\n",
            "step 0049 | STL loss -0.2180 robustness\n",
            "step 0050 | STL loss -0.2180 robustness\n",
            "step 0051 | STL loss -0.2180 robustness\n",
            "step 0052 | STL loss -0.2180 robustness\n",
            "step 0053 | STL loss -0.2180 robustness\n",
            "step 0054 | STL loss -0.2180 robustness\n",
            "step 0055 | STL loss -0.2180 robustness\n",
            "step 0056 | STL loss -0.2180 robustness\n",
            "step 0057 | STL loss -0.2180 robustness\n",
            "step 0058 | STL loss -0.2180 robustness\n",
            "step 0059 | STL loss -0.2180 robustness\n",
            "step 0060 | STL loss -0.2180 robustness\n",
            "step 0061 | STL loss -0.2180 robustness\n",
            "step 0062 | STL loss -0.2180 robustness\n",
            "step 0063 | STL loss -0.2180 robustness\n",
            "step 0064 | STL loss -0.2180 robustness\n",
            "step 0065 | STL loss -0.2180 robustness\n",
            "step 0066 | STL loss -0.2180 robustness\n",
            "step 0067 | STL loss -0.2180 robustness\n",
            "step 0068 | STL loss -0.2180 robustness\n",
            "step 0069 | STL loss -0.2180 robustness\n",
            "step 0070 | STL loss -0.2180 robustness\n",
            "step 0071 | STL loss -0.2180 robustness\n",
            "step 0072 | STL loss -0.2180 robustness\n",
            "step 0073 | STL loss -0.2180 robustness\n",
            "step 0074 | STL loss -0.2180 robustness\n",
            "step 0075 | STL loss -0.2180 robustness\n",
            "step 0076 | STL loss -0.2180 robustness\n",
            "step 0077 | STL loss -0.2180 robustness\n",
            "step 0078 | STL loss -0.2180 robustness\n",
            "step 0079 | STL loss -0.2180 robustness\n",
            "step 0080 | STL loss -0.2180 robustness\n",
            "step 0081 | STL loss -0.2180 robustness\n",
            "step 0082 | STL loss -0.2180 robustness\n",
            "step 0083 | STL loss -0.2180 robustness\n",
            "step 0084 | STL loss -0.2180 robustness\n",
            "step 0085 | STL loss -0.2180 robustness\n",
            "step 0086 | STL loss -0.2180 robustness\n",
            "step 0087 | STL loss -0.2180 robustness\n",
            "step 0088 | STL loss -0.2180 robustness\n",
            "step 0089 | STL loss -0.2180 robustness\n",
            "step 0090 | STL loss -0.2180 robustness\n",
            "step 0091 | STL loss -0.2180 robustness\n",
            "step 0092 | STL loss -0.2180 robustness\n",
            "step 0093 | STL loss -0.2180 robustness\n",
            "step 0094 | STL loss -0.2180 robustness\n",
            "step 0095 | STL loss -0.2180 robustness\n",
            "step 0096 | STL loss -0.2180 robustness\n",
            "step 0097 | STL loss -0.2180 robustness\n",
            "step 0098 | STL loss -0.2180 robustness\n",
            "step 0099 | STL loss -0.2180 robustness\n",
            "step 0100 | STL loss -0.2180 robustness\n",
            "step 0101 | STL loss -0.2180 robustness\n",
            "step 0102 | STL loss -0.2180 robustness\n",
            "step 0103 | STL loss -0.2180 robustness\n",
            "step 0104 | STL loss -0.2180 robustness\n",
            "step 0105 | STL loss -0.2180 robustness\n",
            "step 0106 | STL loss -0.2180 robustness\n",
            "step 0107 | STL loss -0.2180 robustness\n",
            "step 0108 | STL loss -0.2180 robustness\n",
            "step 0109 | STL loss -0.2180 robustness\n",
            "step 0110 | STL loss -0.2180 robustness\n",
            "step 0111 | STL loss -0.2180 robustness\n",
            "step 0112 | STL loss -0.2180 robustness\n",
            "step 0113 | STL loss -0.2180 robustness\n",
            "step 0114 | STL loss -0.2180 robustness\n",
            "step 0115 | STL loss -0.2180 robustness\n",
            "step 0116 | STL loss -0.2180 robustness\n",
            "step 0117 | STL loss -0.2180 robustness\n",
            "step 0118 | STL loss -0.2180 robustness\n",
            "step 0119 | STL loss -0.2180 robustness\n",
            "step 0120 | STL loss -0.2180 robustness\n",
            "step 0121 | STL loss -0.2180 robustness\n",
            "step 0122 | STL loss -0.2180 robustness\n",
            "step 0123 | STL loss -0.2180 robustness\n",
            "step 0124 | STL loss -0.2180 robustness\n",
            "step 0125 | STL loss -0.2180 robustness\n",
            "step 0126 | STL loss -0.2180 robustness\n",
            "step 0127 | STL loss -0.2180 robustness\n",
            "step 0128 | STL loss -0.2180 robustness\n",
            "step 0129 | STL loss -0.2180 robustness\n",
            "step 0130 | STL loss -0.2180 robustness\n",
            "step 0131 | STL loss -0.2180 robustness\n",
            "step 0132 | STL loss -0.2180 robustness\n",
            "step 0133 | STL loss -0.2180 robustness\n",
            "step 0134 | STL loss -0.2180 robustness\n",
            "step 0135 | STL loss -0.2180 robustness\n",
            "step 0136 | STL loss -0.2180 robustness\n",
            "step 0137 | STL loss -0.2180 robustness\n",
            "step 0138 | STL loss -0.2180 robustness\n",
            "step 0139 | STL loss -0.2180 robustness\n",
            "step 0140 | STL loss -0.2180 robustness\n",
            "step 0141 | STL loss -0.2180 robustness\n",
            "step 0142 | STL loss -0.2180 robustness\n",
            "step 0143 | STL loss -0.2180 robustness\n",
            "step 0144 | STL loss -0.2180 robustness\n",
            "step 0145 | STL loss -0.2180 robustness\n",
            "step 0146 | STL loss -0.2180 robustness\n",
            "step 0147 | STL loss -0.2180 robustness\n",
            "step 0148 | STL loss -0.2180 robustness\n",
            "step 0149 | STL loss -0.2180 robustness\n",
            "step 0150 | STL loss -0.2180 robustness\n",
            "step 0151 | STL loss -0.2180 robustness\n",
            "step 0152 | STL loss -0.2180 robustness\n",
            "step 0153 | STL loss -0.2180 robustness\n",
            "step 0154 | STL loss -0.2180 robustness\n",
            "step 0155 | STL loss -0.2180 robustness\n",
            "step 0156 | STL loss -0.2180 robustness\n",
            "step 0157 | STL loss -0.2180 robustness\n",
            "step 0158 | STL loss -0.2180 robustness\n",
            "step 0159 | STL loss -0.2180 robustness\n",
            "step 0160 | STL loss -0.2180 robustness\n",
            "step 0161 | STL loss -0.2180 robustness\n",
            "step 0162 | STL loss -0.2180 robustness\n",
            "step 0163 | STL loss -0.2180 robustness\n",
            "step 0164 | STL loss -0.2180 robustness\n",
            "step 0165 | STL loss -0.2180 robustness\n",
            "step 0166 | STL loss -0.2180 robustness\n",
            "step 0167 | STL loss -0.2180 robustness\n",
            "step 0168 | STL loss -0.2180 robustness\n",
            "step 0169 | STL loss -0.2180 robustness\n",
            "step 0170 | STL loss -0.2180 robustness\n",
            "step 0171 | STL loss -0.2180 robustness\n",
            "step 0172 | STL loss -0.2180 robustness\n",
            "step 0173 | STL loss -0.2180 robustness\n",
            "step 0174 | STL loss -0.2180 robustness\n",
            "step 0175 | STL loss -0.2180 robustness\n",
            "step 0176 | STL loss -0.2180 robustness\n",
            "step 0177 | STL loss -0.2180 robustness\n",
            "step 0178 | STL loss -0.2180 robustness\n",
            "step 0179 | STL loss -0.2180 robustness\n",
            "step 0180 | STL loss -0.2180 robustness\n",
            "step 0181 | STL loss -0.2180 robustness\n",
            "step 0182 | STL loss -0.2180 robustness\n",
            "step 0183 | STL loss -0.2180 robustness\n",
            "step 0184 | STL loss -0.2180 robustness\n",
            "step 0185 | STL loss -0.2180 robustness\n",
            "step 0186 | STL loss -0.2180 robustness\n",
            "step 0187 | STL loss -0.2180 robustness\n",
            "step 0188 | STL loss -0.2180 robustness\n",
            "step 0189 | STL loss -0.2180 robustness\n",
            "step 0190 | STL loss -0.2180 robustness\n",
            "step 0191 | STL loss -0.2180 robustness\n",
            "step 0192 | STL loss -0.2180 robustness\n",
            "step 0193 | STL loss -0.2180 robustness\n",
            "step 0194 | STL loss -0.2180 robustness\n",
            "step 0195 | STL loss -0.2180 robustness\n",
            "step 0196 | STL loss -0.2180 robustness\n",
            "step 0197 | STL loss -0.2180 robustness\n",
            "step 0198 | STL loss -0.2180 robustness\n",
            "step 0199 | STL loss -0.2180 robustness\n",
            "step 0200 | STL loss -0.2180 robustness\n",
            "step 0201 | STL loss -0.2180 robustness\n",
            "step 0202 | STL loss -0.2180 robustness\n",
            "step 0203 | STL loss -0.2180 robustness\n",
            "step 0204 | STL loss -0.2180 robustness\n",
            "step 0205 | STL loss -0.2180 robustness\n",
            "step 0206 | STL loss -0.2180 robustness\n",
            "step 0207 | STL loss -0.2180 robustness\n",
            "step 0208 | STL loss -0.2180 robustness\n",
            "step 0209 | STL loss -0.2180 robustness\n",
            "step 0210 | STL loss -0.2180 robustness\n",
            "step 0211 | STL loss -0.2180 robustness\n",
            "step 0212 | STL loss -0.2180 robustness\n",
            "step 0213 | STL loss -0.2180 robustness\n",
            "step 0214 | STL loss -0.2180 robustness\n",
            "step 0215 | STL loss -0.2180 robustness\n",
            "step 0216 | STL loss -0.2180 robustness\n",
            "step 0217 | STL loss -0.2180 robustness\n",
            "step 0218 | STL loss -0.2180 robustness\n",
            "step 0219 | STL loss -0.2180 robustness\n",
            "step 0220 | STL loss -0.2180 robustness\n",
            "step 0221 | STL loss -0.2180 robustness\n",
            "step 0222 | STL loss -0.2180 robustness\n",
            "step 0223 | STL loss -0.2180 robustness\n",
            "step 0224 | STL loss -0.2180 robustness\n",
            "step 0225 | STL loss -0.2180 robustness\n",
            "step 0226 | STL loss -0.2180 robustness\n",
            "step 0227 | STL loss -0.2180 robustness\n",
            "step 0228 | STL loss -0.2180 robustness\n",
            "step 0229 | STL loss -0.2180 robustness\n",
            "step 0230 | STL loss -0.2180 robustness\n",
            "step 0231 | STL loss -0.2180 robustness\n",
            "step 0232 | STL loss -0.2180 robustness\n",
            "step 0233 | STL loss -0.2180 robustness\n",
            "step 0234 | STL loss -0.2180 robustness\n",
            "step 0235 | STL loss -0.2180 robustness\n",
            "step 0236 | STL loss -0.2180 robustness\n",
            "step 0237 | STL loss -0.2180 robustness\n",
            "step 0238 | STL loss -0.2180 robustness\n",
            "step 0239 | STL loss -0.2180 robustness\n",
            "step 0240 | STL loss -0.2180 robustness\n",
            "step 0241 | STL loss -0.2180 robustness\n",
            "step 0242 | STL loss -0.2180 robustness\n",
            "step 0243 | STL loss -0.2180 robustness\n",
            "step 0244 | STL loss -0.2180 robustness\n",
            "step 0245 | STL loss -0.2180 robustness\n",
            "step 0246 | STL loss -0.2180 robustness\n",
            "step 0247 | STL loss -0.2180 robustness\n",
            "step 0248 | STL loss -0.2180 robustness\n",
            "step 0249 | STL loss -0.2180 robustness\n",
            "step 0250 | STL loss -0.2180 robustness\n",
            "step 0251 | STL loss -0.2180 robustness\n",
            "step 0252 | STL loss -0.2180 robustness\n",
            "step 0253 | STL loss -0.2180 robustness\n",
            "step 0254 | STL loss -0.2180 robustness\n",
            "step 0255 | STL loss -0.2180 robustness\n",
            "step 0256 | STL loss -0.2180 robustness\n",
            "step 0257 | STL loss -0.2180 robustness\n",
            "step 0258 | STL loss -0.2180 robustness\n",
            "step 0259 | STL loss -0.2180 robustness\n",
            "step 0260 | STL loss -0.2180 robustness\n",
            "step 0261 | STL loss -0.2180 robustness\n",
            "step 0262 | STL loss -0.2180 robustness\n",
            "step 0263 | STL loss -0.2180 robustness\n",
            "step 0264 | STL loss -0.2180 robustness\n",
            "step 0265 | STL loss -0.2180 robustness\n",
            "step 0266 | STL loss -0.2180 robustness\n",
            "step 0267 | STL loss -0.2180 robustness\n",
            "step 0268 | STL loss -0.2180 robustness\n",
            "step 0269 | STL loss -0.2180 robustness\n",
            "step 0270 | STL loss -0.2180 robustness\n",
            "step 0271 | STL loss -0.2180 robustness\n",
            "step 0272 | STL loss -0.2180 robustness\n",
            "step 0273 | STL loss -0.2180 robustness\n",
            "step 0274 | STL loss -0.2180 robustness\n",
            "step 0275 | STL loss -0.2180 robustness\n",
            "step 0276 | STL loss -0.2180 robustness\n",
            "step 0277 | STL loss -0.2180 robustness\n",
            "step 0278 | STL loss -0.2180 robustness\n",
            "step 0279 | STL loss -0.2180 robustness\n",
            "step 0280 | STL loss -0.2180 robustness\n",
            "step 0281 | STL loss -0.2180 robustness\n",
            "step 0282 | STL loss -0.2180 robustness\n",
            "step 0283 | STL loss -0.2180 robustness\n",
            "step 0284 | STL loss -0.2180 robustness\n",
            "step 0285 | STL loss -0.2180 robustness\n",
            "step 0286 | STL loss -0.2180 robustness\n",
            "step 0287 | STL loss -0.2180 robustness\n",
            "step 0288 | STL loss -0.2180 robustness\n",
            "step 0289 | STL loss -0.2180 robustness\n",
            "step 0290 | STL loss -0.2180 robustness\n",
            "step 0291 | STL loss -0.2180 robustness\n",
            "step 0292 | STL loss -0.2180 robustness\n",
            "step 0293 | STL loss -0.2180 robustness\n",
            "step 0294 | STL loss -0.2180 robustness\n",
            "step 0295 | STL loss -0.2180 robustness\n",
            "step 0296 | STL loss -0.2180 robustness\n",
            "step 0297 | STL loss -0.2180 robustness\n",
            "step 0298 | STL loss -0.2180 robustness\n",
            "step 0299 | STL loss -0.2180 robustness\n",
            "step 0300 | STL loss -0.2180 robustness\n",
            "step 0301 | STL loss -0.2180 robustness\n",
            "step 0302 | STL loss -0.2180 robustness\n",
            "step 0303 | STL loss -0.2180 robustness\n",
            "step 0304 | STL loss -0.2180 robustness\n",
            "step 0305 | STL loss -0.2180 robustness\n",
            "step 0306 | STL loss -0.2180 robustness\n",
            "step 0307 | STL loss -0.2180 robustness\n",
            "step 0308 | STL loss -0.2180 robustness\n",
            "step 0309 | STL loss -0.2180 robustness\n",
            "step 0310 | STL loss -0.2180 robustness\n",
            "step 0311 | STL loss -0.2180 robustness\n",
            "step 0312 | STL loss -0.2180 robustness\n",
            "step 0313 | STL loss -0.2180 robustness\n",
            "step 0314 | STL loss -0.2180 robustness\n",
            "step 0315 | STL loss -0.2180 robustness\n",
            "step 0316 | STL loss -0.2180 robustness\n",
            "step 0317 | STL loss -0.2180 robustness\n",
            "step 0318 | STL loss -0.2180 robustness\n",
            "step 0319 | STL loss -0.2180 robustness\n",
            "step 0320 | STL loss -0.2180 robustness\n",
            "step 0321 | STL loss -0.2180 robustness\n",
            "step 0322 | STL loss -0.2180 robustness\n",
            "step 0323 | STL loss -0.2180 robustness\n",
            "step 0324 | STL loss -0.2180 robustness\n",
            "step 0325 | STL loss -0.2180 robustness\n",
            "step 0326 | STL loss -0.2180 robustness\n",
            "step 0327 | STL loss -0.2180 robustness\n",
            "step 0328 | STL loss -0.2180 robustness\n",
            "step 0329 | STL loss -0.2180 robustness\n",
            "step 0330 | STL loss -0.2180 robustness\n",
            "step 0331 | STL loss -0.2180 robustness\n",
            "step 0332 | STL loss -0.2180 robustness\n",
            "step 0333 | STL loss -0.2180 robustness\n",
            "step 0334 | STL loss -0.2180 robustness\n",
            "step 0335 | STL loss -0.2180 robustness\n",
            "step 0336 | STL loss -0.2180 robustness\n",
            "step 0337 | STL loss -0.2180 robustness\n",
            "step 0338 | STL loss -0.2180 robustness\n",
            "step 0339 | STL loss -0.2180 robustness\n",
            "step 0340 | STL loss -0.2180 robustness\n",
            "step 0341 | STL loss -0.2180 robustness\n",
            "step 0342 | STL loss -0.2180 robustness\n",
            "step 0343 | STL loss -0.2180 robustness\n",
            "step 0344 | STL loss -0.2180 robustness\n",
            "step 0345 | STL loss -0.2180 robustness\n",
            "step 0346 | STL loss -0.2180 robustness\n",
            "step 0347 | STL loss -0.2180 robustness\n",
            "step 0348 | STL loss -0.2180 robustness\n",
            "step 0349 | STL loss -0.2180 robustness\n",
            "step 0350 | STL loss -0.2180 robustness\n",
            "step 0351 | STL loss -0.2180 robustness\n",
            "step 0352 | STL loss -0.2180 robustness\n",
            "step 0353 | STL loss -0.2180 robustness\n",
            "step 0354 | STL loss -0.2180 robustness\n",
            "step 0355 | STL loss -0.2180 robustness\n",
            "step 0356 | STL loss -0.2180 robustness\n",
            "step 0357 | STL loss -0.2180 robustness\n",
            "step 0358 | STL loss -0.2180 robustness\n",
            "step 0359 | STL loss -0.2180 robustness\n",
            "step 0360 | STL loss -0.2180 robustness\n",
            "step 0361 | STL loss -0.2180 robustness\n",
            "step 0362 | STL loss -0.2180 robustness\n",
            "step 0363 | STL loss -0.2180 robustness\n",
            "step 0364 | STL loss -0.2180 robustness\n",
            "step 0365 | STL loss -0.2180 robustness\n",
            "step 0366 | STL loss -0.2180 robustness\n",
            "step 0367 | STL loss -0.2180 robustness\n",
            "step 0368 | STL loss -0.2180 robustness\n",
            "step 0369 | STL loss -0.2180 robustness\n",
            "step 0370 | STL loss -0.2180 robustness\n",
            "step 0371 | STL loss -0.2180 robustness\n",
            "step 0372 | STL loss -0.2180 robustness\n",
            "step 0373 | STL loss -0.2180 robustness\n",
            "step 0374 | STL loss -0.2180 robustness\n",
            "step 0375 | STL loss -0.2180 robustness\n",
            "step 0376 | STL loss -0.2180 robustness\n",
            "step 0377 | STL loss -0.2180 robustness\n",
            "step 0378 | STL loss -0.2180 robustness\n",
            "step 0379 | STL loss -0.2180 robustness\n",
            "step 0380 | STL loss -0.2180 robustness\n",
            "step 0381 | STL loss -0.2180 robustness\n",
            "step 0382 | STL loss -0.2180 robustness\n",
            "step 0383 | STL loss -0.2180 robustness\n",
            "step 0384 | STL loss -0.2180 robustness\n",
            "step 0385 | STL loss -0.2180 robustness\n",
            "step 0386 | STL loss -0.2180 robustness\n",
            "step 0387 | STL loss -0.2180 robustness\n",
            "step 0388 | STL loss -0.2180 robustness\n",
            "step 0389 | STL loss -0.2180 robustness\n",
            "step 0390 | STL loss -0.2180 robustness\n",
            "step 0391 | STL loss -0.2180 robustness\n",
            "step 0392 | STL loss -0.2180 robustness\n",
            "step 0393 | STL loss -0.2180 robustness\n",
            "step 0394 | STL loss -0.2180 robustness\n",
            "step 0395 | STL loss -0.2180 robustness\n",
            "step 0396 | STL loss -0.2180 robustness\n",
            "step 0397 | STL loss -0.2180 robustness\n",
            "step 0398 | STL loss -0.2180 robustness\n",
            "step 0399 | STL loss -0.2180 robustness\n",
            "step 0400 | STL loss -0.2180 robustness\n",
            "step 0401 | STL loss -0.2180 robustness\n",
            "step 0402 | STL loss -0.2180 robustness\n",
            "step 0403 | STL loss -0.2180 robustness\n",
            "step 0404 | STL loss -0.2180 robustness\n",
            "step 0405 | STL loss -0.2180 robustness\n",
            "step 0406 | STL loss -0.2180 robustness\n",
            "step 0407 | STL loss -0.2180 robustness\n",
            "step 0408 | STL loss -0.2180 robustness\n",
            "step 0409 | STL loss -0.2180 robustness\n",
            "step 0410 | STL loss -0.2180 robustness\n",
            "step 0411 | STL loss -0.2180 robustness\n",
            "step 0412 | STL loss -0.2180 robustness\n",
            "step 0413 | STL loss -0.2180 robustness\n",
            "step 0414 | STL loss -0.2180 robustness\n",
            "step 0415 | STL loss -0.2180 robustness\n",
            "step 0416 | STL loss -0.2180 robustness\n",
            "step 0417 | STL loss -0.2180 robustness\n",
            "step 0418 | STL loss -0.2180 robustness\n",
            "step 0419 | STL loss -0.2180 robustness\n",
            "step 0420 | STL loss -0.2180 robustness\n",
            "step 0421 | STL loss -0.2180 robustness\n",
            "step 0422 | STL loss -0.2180 robustness\n",
            "step 0423 | STL loss -0.2180 robustness\n",
            "step 0424 | STL loss -0.2180 robustness\n",
            "step 0425 | STL loss -0.2180 robustness\n",
            "step 0426 | STL loss -0.2180 robustness\n",
            "step 0427 | STL loss -0.2180 robustness\n",
            "step 0428 | STL loss -0.2180 robustness\n",
            "step 0429 | STL loss -0.2180 robustness\n",
            "step 0430 | STL loss -0.2180 robustness\n",
            "step 0431 | STL loss -0.2180 robustness\n",
            "step 0432 | STL loss -0.2180 robustness\n",
            "step 0433 | STL loss -0.2180 robustness\n",
            "step 0434 | STL loss -0.2180 robustness\n",
            "step 0435 | STL loss -0.2180 robustness\n",
            "step 0436 | STL loss -0.2180 robustness\n",
            "step 0437 | STL loss -0.2180 robustness\n",
            "step 0438 | STL loss -0.2180 robustness\n",
            "step 0439 | STL loss -0.2180 robustness\n",
            "step 0440 | STL loss -0.2180 robustness\n",
            "step 0441 | STL loss -0.2180 robustness\n",
            "step 0442 | STL loss -0.2180 robustness\n",
            "step 0443 | STL loss -0.2180 robustness\n",
            "step 0444 | STL loss -0.2180 robustness\n",
            "step 0445 | STL loss -0.2180 robustness\n",
            "step 0446 | STL loss -0.2180 robustness\n",
            "step 0447 | STL loss -0.2180 robustness\n",
            "step 0448 | STL loss -0.2180 robustness\n",
            "step 0449 | STL loss -0.2180 robustness\n",
            "step 0450 | STL loss -0.2180 robustness\n",
            "step 0451 | STL loss -0.2180 robustness\n",
            "step 0452 | STL loss -0.2180 robustness\n",
            "step 0453 | STL loss -0.2180 robustness\n",
            "step 0454 | STL loss -0.2180 robustness\n",
            "step 0455 | STL loss -0.2180 robustness\n",
            "step 0456 | STL loss -0.2180 robustness\n",
            "step 0457 | STL loss -0.2180 robustness\n",
            "step 0458 | STL loss -0.2180 robustness\n",
            "step 0459 | STL loss -0.2180 robustness\n",
            "step 0460 | STL loss -0.2180 robustness\n",
            "step 0461 | STL loss -0.2180 robustness\n",
            "step 0462 | STL loss -0.2180 robustness\n",
            "step 0463 | STL loss -0.2180 robustness\n",
            "step 0464 | STL loss -0.2180 robustness\n",
            "step 0465 | STL loss -0.2180 robustness\n",
            "step 0466 | STL loss -0.2180 robustness\n",
            "step 0467 | STL loss -0.2180 robustness\n",
            "step 0468 | STL loss -0.2180 robustness\n",
            "step 0469 | STL loss -0.2180 robustness\n",
            "step 0470 | STL loss -0.2180 robustness\n",
            "step 0471 | STL loss -0.2180 robustness\n",
            "step 0472 | STL loss -0.2180 robustness\n",
            "step 0473 | STL loss -0.2180 robustness\n",
            "step 0474 | STL loss -0.2180 robustness\n",
            "step 0475 | STL loss -0.2180 robustness\n",
            "step 0476 | STL loss -0.2180 robustness\n",
            "step 0477 | STL loss -0.2180 robustness\n",
            "step 0478 | STL loss -0.2180 robustness\n",
            "step 0479 | STL loss -0.2180 robustness\n",
            "step 0480 | STL loss -0.2180 robustness\n",
            "step 0481 | STL loss -0.2180 robustness\n",
            "step 0482 | STL loss -0.2180 robustness\n",
            "step 0483 | STL loss -0.2180 robustness\n",
            "step 0484 | STL loss -0.2180 robustness\n",
            "step 0485 | STL loss -0.2180 robustness\n",
            "step 0486 | STL loss -0.2180 robustness\n",
            "step 0487 | STL loss -0.2180 robustness\n",
            "step 0488 | STL loss -0.2180 robustness\n",
            "step 0489 | STL loss -0.2180 robustness\n",
            "step 0490 | STL loss -0.2180 robustness\n",
            "step 0491 | STL loss -0.2180 robustness\n",
            "step 0492 | STL loss -0.2180 robustness\n",
            "step 0493 | STL loss -0.2180 robustness\n",
            "step 0494 | STL loss -0.2180 robustness\n",
            "step 0495 | STL loss -0.2180 robustness\n",
            "step 0496 | STL loss -0.2180 robustness\n",
            "step 0497 | STL loss -0.2180 robustness\n",
            "step 0498 | STL loss -0.2180 robustness\n",
            "step 0499 | STL loss -0.2180 robustness\n",
            "step 0500 | STL loss -0.2180 robustness\n",
            "step 0501 | STL loss -0.2180 robustness\n",
            "step 0502 | STL loss -0.2180 robustness\n",
            "step 0503 | STL loss -0.2180 robustness\n",
            "step 0504 | STL loss -0.2180 robustness\n",
            "step 0505 | STL loss -0.2180 robustness\n",
            "step 0506 | STL loss -0.2180 robustness\n",
            "step 0507 | STL loss -0.2180 robustness\n",
            "step 0508 | STL loss -0.2180 robustness\n",
            "step 0509 | STL loss -0.2180 robustness\n",
            "step 0510 | STL loss -0.2180 robustness\n",
            "step 0511 | STL loss -0.2180 robustness\n",
            "step 0512 | STL loss -0.2180 robustness\n",
            "step 0513 | STL loss -0.2180 robustness\n",
            "step 0514 | STL loss -0.2180 robustness\n",
            "step 0515 | STL loss -0.2180 robustness\n",
            "step 0516 | STL loss -0.2180 robustness\n",
            "step 0517 | STL loss -0.2180 robustness\n",
            "step 0518 | STL loss -0.2180 robustness\n",
            "step 0519 | STL loss -0.2180 robustness\n",
            "step 0520 | STL loss -0.2180 robustness\n",
            "step 0521 | STL loss -0.2180 robustness\n",
            "step 0522 | STL loss -0.2180 robustness\n",
            "step 0523 | STL loss -0.2180 robustness\n",
            "step 0524 | STL loss -0.2180 robustness\n",
            "step 0525 | STL loss -0.2180 robustness\n",
            "step 0526 | STL loss -0.2180 robustness\n",
            "step 0527 | STL loss -0.2180 robustness\n",
            "step 0528 | STL loss -0.2180 robustness\n",
            "step 0529 | STL loss -0.2180 robustness\n",
            "step 0530 | STL loss -0.2180 robustness\n",
            "step 0531 | STL loss -0.2180 robustness\n",
            "step 0532 | STL loss -0.2180 robustness\n",
            "step 0533 | STL loss -0.2180 robustness\n",
            "step 0534 | STL loss -0.2180 robustness\n",
            "step 0535 | STL loss -0.2180 robustness\n",
            "step 0536 | STL loss -0.2180 robustness\n",
            "step 0537 | STL loss -0.2180 robustness\n",
            "step 0538 | STL loss -0.2180 robustness\n",
            "step 0539 | STL loss -0.2180 robustness\n",
            "step 0540 | STL loss -0.2180 robustness\n",
            "step 0541 | STL loss -0.2180 robustness\n",
            "step 0542 | STL loss -0.2180 robustness\n",
            "step 0543 | STL loss -0.2180 robustness\n",
            "step 0544 | STL loss -0.2180 robustness\n",
            "step 0545 | STL loss -0.2180 robustness\n",
            "step 0546 | STL loss -0.2180 robustness\n",
            "step 0547 | STL loss -0.2180 robustness\n",
            "step 0548 | STL loss -0.2180 robustness\n",
            "step 0549 | STL loss -0.2180 robustness\n",
            "step 0550 | STL loss -0.2180 robustness\n",
            "step 0551 | STL loss -0.2180 robustness\n",
            "step 0552 | STL loss -0.2180 robustness\n",
            "step 0553 | STL loss -0.2180 robustness\n",
            "step 0554 | STL loss -0.2180 robustness\n",
            "step 0555 | STL loss -0.2180 robustness\n",
            "step 0556 | STL loss -0.2180 robustness\n",
            "step 0557 | STL loss -0.2180 robustness\n",
            "step 0558 | STL loss -0.2180 robustness\n",
            "step 0559 | STL loss -0.2180 robustness\n",
            "step 0560 | STL loss -0.2180 robustness\n",
            "step 0561 | STL loss -0.2180 robustness\n",
            "step 0562 | STL loss -0.2180 robustness\n",
            "step 0563 | STL loss -0.2180 robustness\n",
            "step 0564 | STL loss -0.2180 robustness\n",
            "step 0565 | STL loss -0.2180 robustness\n",
            "step 0566 | STL loss -0.2180 robustness\n",
            "step 0567 | STL loss -0.2180 robustness\n",
            "step 0568 | STL loss -0.2180 robustness\n",
            "step 0569 | STL loss -0.2180 robustness\n",
            "step 0570 | STL loss -0.2180 robustness\n",
            "step 0571 | STL loss -0.2180 robustness\n",
            "step 0572 | STL loss -0.2180 robustness\n",
            "step 0573 | STL loss -0.2180 robustness\n",
            "step 0574 | STL loss -0.2180 robustness\n",
            "step 0575 | STL loss -0.2180 robustness\n",
            "step 0576 | STL loss -0.2180 robustness\n",
            "step 0577 | STL loss -0.2180 robustness\n",
            "step 0578 | STL loss -0.2180 robustness\n",
            "step 0579 | STL loss -0.2180 robustness\n",
            "step 0580 | STL loss -0.2180 robustness\n",
            "step 0581 | STL loss -0.2180 robustness\n",
            "step 0582 | STL loss -0.2180 robustness\n",
            "step 0583 | STL loss -0.2180 robustness\n",
            "step 0584 | STL loss -0.2180 robustness\n",
            "step 0585 | STL loss -0.2180 robustness\n",
            "step 0586 | STL loss -0.2180 robustness\n",
            "step 0587 | STL loss -0.2180 robustness\n",
            "step 0588 | STL loss -0.2180 robustness\n",
            "step 0589 | STL loss -0.2180 robustness\n",
            "step 0590 | STL loss -0.2180 robustness\n",
            "step 0591 | STL loss -0.2180 robustness\n",
            "step 0592 | STL loss -0.2180 robustness\n",
            "step 0593 | STL loss -0.2180 robustness\n",
            "step 0594 | STL loss -0.2180 robustness\n",
            "step 0595 | STL loss -0.2180 robustness\n",
            "step 0596 | STL loss -0.2180 robustness\n",
            "step 0597 | STL loss -0.2180 robustness\n",
            "step 0598 | STL loss -0.2180 robustness\n",
            "step 0599 | STL loss -0.2180 robustness\n",
            "step 0600 | STL loss -0.2180 robustness\n",
            "step 0601 | STL loss -0.2180 robustness\n",
            "step 0602 | STL loss -0.2180 robustness\n",
            "step 0603 | STL loss -0.2180 robustness\n",
            "step 0604 | STL loss -0.2180 robustness\n",
            "step 0605 | STL loss -0.2180 robustness\n",
            "step 0606 | STL loss -0.2180 robustness\n",
            "step 0607 | STL loss -0.2180 robustness\n",
            "step 0608 | STL loss -0.2180 robustness\n",
            "step 0609 | STL loss -0.2180 robustness\n",
            "step 0610 | STL loss -0.2180 robustness\n",
            "step 0611 | STL loss -0.2180 robustness\n",
            "step 0612 | STL loss -0.2180 robustness\n",
            "step 0613 | STL loss -0.2180 robustness\n",
            "step 0614 | STL loss -0.2180 robustness\n",
            "step 0615 | STL loss -0.2180 robustness\n",
            "step 0616 | STL loss -0.2180 robustness\n",
            "step 0617 | STL loss -0.2180 robustness\n",
            "step 0618 | STL loss -0.2180 robustness\n",
            "step 0619 | STL loss -0.2180 robustness\n",
            "step 0620 | STL loss -0.2180 robustness\n",
            "step 0621 | STL loss -0.2180 robustness\n",
            "step 0622 | STL loss -0.2180 robustness\n",
            "step 0623 | STL loss -0.2180 robustness\n",
            "step 0624 | STL loss -0.2180 robustness\n",
            "step 0625 | STL loss -0.2180 robustness\n",
            "step 0626 | STL loss -0.2180 robustness\n",
            "step 0627 | STL loss -0.2180 robustness\n",
            "step 0628 | STL loss -0.2180 robustness\n",
            "step 0629 | STL loss -0.2180 robustness\n",
            "step 0630 | STL loss -0.2180 robustness\n",
            "step 0631 | STL loss -0.2180 robustness\n",
            "step 0632 | STL loss -0.2180 robustness\n",
            "step 0633 | STL loss -0.2180 robustness\n",
            "step 0634 | STL loss -0.2180 robustness\n",
            "step 0635 | STL loss -0.2180 robustness\n",
            "step 0636 | STL loss -0.2180 robustness\n",
            "step 0637 | STL loss -0.2180 robustness\n",
            "step 0638 | STL loss -0.2180 robustness\n",
            "step 0639 | STL loss -0.2180 robustness\n",
            "step 0640 | STL loss -0.2180 robustness\n",
            "step 0641 | STL loss -0.2180 robustness\n",
            "step 0642 | STL loss -0.2180 robustness\n",
            "step 0643 | STL loss -0.2180 robustness\n",
            "step 0644 | STL loss -0.2180 robustness\n",
            "step 0645 | STL loss -0.2180 robustness\n",
            "step 0646 | STL loss -0.2180 robustness\n",
            "step 0647 | STL loss -0.2180 robustness\n",
            "step 0648 | STL loss -0.2180 robustness\n",
            "step 0649 | STL loss -0.2180 robustness\n",
            "step 0650 | STL loss -0.2180 robustness\n",
            "step 0651 | STL loss -0.2180 robustness\n",
            "step 0652 | STL loss -0.2180 robustness\n",
            "step 0653 | STL loss -0.2180 robustness\n",
            "step 0654 | STL loss -0.2180 robustness\n",
            "step 0655 | STL loss -0.2180 robustness\n",
            "step 0656 | STL loss -0.2180 robustness\n",
            "step 0657 | STL loss -0.2180 robustness\n",
            "step 0658 | STL loss -0.2180 robustness\n",
            "step 0659 | STL loss -0.2180 robustness\n",
            "step 0660 | STL loss -0.2180 robustness\n",
            "step 0661 | STL loss -0.2180 robustness\n",
            "step 0662 | STL loss -0.2180 robustness\n",
            "step 0663 | STL loss -0.2180 robustness\n",
            "step 0664 | STL loss -0.2180 robustness\n",
            "step 0665 | STL loss -0.2180 robustness\n",
            "step 0666 | STL loss -0.2180 robustness\n",
            "step 0667 | STL loss -0.2180 robustness\n",
            "step 0668 | STL loss -0.2180 robustness\n",
            "step 0669 | STL loss -0.2180 robustness\n",
            "step 0670 | STL loss -0.2180 robustness\n",
            "step 0671 | STL loss -0.2180 robustness\n",
            "step 0672 | STL loss -0.2180 robustness\n",
            "step 0673 | STL loss -0.2180 robustness\n",
            "step 0674 | STL loss -0.2180 robustness\n",
            "step 0675 | STL loss -0.2180 robustness\n",
            "step 0676 | STL loss -0.2180 robustness\n",
            "step 0677 | STL loss -0.2180 robustness\n",
            "step 0678 | STL loss -0.2180 robustness\n",
            "step 0679 | STL loss -0.2180 robustness\n",
            "step 0680 | STL loss -0.2180 robustness\n",
            "step 0681 | STL loss -0.2180 robustness\n",
            "step 0682 | STL loss -0.2180 robustness\n",
            "step 0683 | STL loss -0.2180 robustness\n",
            "step 0684 | STL loss -0.2180 robustness\n",
            "step 0685 | STL loss -0.2180 robustness\n",
            "step 0686 | STL loss -0.2180 robustness\n",
            "step 0687 | STL loss -0.2180 robustness\n",
            "step 0688 | STL loss -0.2180 robustness\n",
            "step 0689 | STL loss -0.2180 robustness\n",
            "step 0690 | STL loss -0.2180 robustness\n",
            "step 0691 | STL loss -0.2180 robustness\n",
            "step 0692 | STL loss -0.2180 robustness\n",
            "step 0693 | STL loss -0.2180 robustness\n",
            "step 0694 | STL loss -0.2180 robustness\n",
            "step 0695 | STL loss -0.2180 robustness\n",
            "step 0696 | STL loss -0.2180 robustness\n",
            "step 0697 | STL loss -0.2180 robustness\n",
            "step 0698 | STL loss -0.2180 robustness\n",
            "step 0699 | STL loss -0.2180 robustness\n",
            "step 0700 | STL loss -0.2180 robustness\n",
            "step 0701 | STL loss -0.2180 robustness\n",
            "step 0702 | STL loss -0.2180 robustness\n",
            "step 0703 | STL loss -0.2180 robustness\n",
            "step 0704 | STL loss -0.2180 robustness\n",
            "step 0705 | STL loss -0.2180 robustness\n",
            "step 0706 | STL loss -0.2180 robustness\n",
            "step 0707 | STL loss -0.2180 robustness\n",
            "step 0708 | STL loss -0.2180 robustness\n",
            "step 0709 | STL loss -0.2180 robustness\n",
            "step 0710 | STL loss -0.2180 robustness\n",
            "step 0711 | STL loss -0.2180 robustness\n",
            "step 0712 | STL loss -0.2180 robustness\n",
            "step 0713 | STL loss -0.2180 robustness\n",
            "step 0714 | STL loss -0.2180 robustness\n",
            "step 0715 | STL loss -0.2180 robustness\n",
            "step 0716 | STL loss -0.2180 robustness\n",
            "step 0717 | STL loss -0.2180 robustness\n",
            "step 0718 | STL loss -0.2180 robustness\n",
            "step 0719 | STL loss -0.2180 robustness\n",
            "step 0720 | STL loss -0.2180 robustness\n",
            "step 0721 | STL loss -0.2180 robustness\n",
            "step 0722 | STL loss -0.2180 robustness\n",
            "step 0723 | STL loss -0.2180 robustness\n",
            "step 0724 | STL loss -0.2180 robustness\n",
            "step 0725 | STL loss -0.2180 robustness\n",
            "step 0726 | STL loss -0.2180 robustness\n",
            "step 0727 | STL loss -0.2180 robustness\n",
            "step 0728 | STL loss -0.2180 robustness\n",
            "step 0729 | STL loss -0.2180 robustness\n",
            "step 0730 | STL loss -0.2180 robustness\n",
            "step 0731 | STL loss -0.2180 robustness\n",
            "step 0732 | STL loss -0.2180 robustness\n",
            "step 0733 | STL loss -0.2180 robustness\n",
            "step 0734 | STL loss -0.2180 robustness\n",
            "step 0735 | STL loss -0.2180 robustness\n",
            "step 0736 | STL loss -0.2180 robustness\n",
            "step 0737 | STL loss -0.2180 robustness\n",
            "step 0738 | STL loss -0.2180 robustness\n",
            "step 0739 | STL loss -0.2180 robustness\n",
            "step 0740 | STL loss -0.2180 robustness\n",
            "step 0741 | STL loss -0.2180 robustness\n",
            "step 0742 | STL loss -0.2180 robustness\n",
            "step 0743 | STL loss -0.2180 robustness\n",
            "step 0744 | STL loss -0.2180 robustness\n",
            "step 0745 | STL loss -0.2180 robustness\n",
            "step 0746 | STL loss -0.2180 robustness\n",
            "step 0747 | STL loss -0.2180 robustness\n",
            "step 0748 | STL loss -0.2180 robustness\n",
            "step 0749 | STL loss -0.2180 robustness\n",
            "step 0750 | STL loss -0.2180 robustness\n",
            "step 0751 | STL loss -0.2180 robustness\n",
            "step 0752 | STL loss -0.2180 robustness\n",
            "step 0753 | STL loss -0.2180 robustness\n",
            "step 0754 | STL loss -0.2180 robustness\n",
            "step 0755 | STL loss -0.2180 robustness\n",
            "step 0756 | STL loss -0.2180 robustness\n",
            "step 0757 | STL loss -0.2180 robustness\n",
            "step 0758 | STL loss -0.2180 robustness\n",
            "step 0759 | STL loss -0.2180 robustness\n",
            "step 0760 | STL loss -0.2180 robustness\n",
            "step 0761 | STL loss -0.2180 robustness\n",
            "step 0762 | STL loss -0.2180 robustness\n",
            "step 0763 | STL loss -0.2180 robustness\n",
            "step 0764 | STL loss -0.2180 robustness\n",
            "step 0765 | STL loss -0.2180 robustness\n",
            "step 0766 | STL loss -0.2180 robustness\n",
            "step 0767 | STL loss -0.2180 robustness\n",
            "step 0768 | STL loss -0.2180 robustness\n",
            "step 0769 | STL loss -0.2180 robustness\n",
            "step 0770 | STL loss -0.2180 robustness\n",
            "step 0771 | STL loss -0.2180 robustness\n",
            "step 0772 | STL loss -0.2180 robustness\n",
            "step 0773 | STL loss -0.2180 robustness\n",
            "step 0774 | STL loss -0.2180 robustness\n",
            "step 0775 | STL loss -0.2180 robustness\n",
            "step 0776 | STL loss -0.2180 robustness\n",
            "step 0777 | STL loss -0.2180 robustness\n",
            "step 0778 | STL loss -0.2180 robustness\n",
            "step 0779 | STL loss -0.2180 robustness\n",
            "step 0780 | STL loss -0.2180 robustness\n",
            "step 0781 | STL loss -0.2180 robustness\n",
            "step 0782 | STL loss -0.2180 robustness\n",
            "step 0783 | STL loss -0.2180 robustness\n",
            "step 0784 | STL loss -0.2180 robustness\n",
            "step 0785 | STL loss -0.2180 robustness\n",
            "step 0786 | STL loss -0.2180 robustness\n",
            "step 0787 | STL loss -0.2180 robustness\n",
            "step 0788 | STL loss -0.2180 robustness\n",
            "step 0789 | STL loss -0.2180 robustness\n",
            "step 0790 | STL loss -0.2180 robustness\n",
            "step 0791 | STL loss -0.2180 robustness\n",
            "step 0792 | STL loss -0.2180 robustness\n",
            "step 0793 | STL loss -0.2180 robustness\n",
            "step 0794 | STL loss -0.2180 robustness\n",
            "step 0795 | STL loss -0.2180 robustness\n",
            "step 0796 | STL loss -0.2180 robustness\n",
            "step 0797 | STL loss -0.2180 robustness\n",
            "step 0798 | STL loss -0.2180 robustness\n",
            "step 0799 | STL loss -0.2180 robustness\n",
            "step 0800 | STL loss -0.2180 robustness\n",
            "step 0801 | STL loss -0.2180 robustness\n",
            "step 0802 | STL loss -0.2180 robustness\n",
            "step 0803 | STL loss -0.2180 robustness\n",
            "step 0804 | STL loss -0.2180 robustness\n",
            "step 0805 | STL loss -0.2180 robustness\n",
            "step 0806 | STL loss -0.2180 robustness\n",
            "step 0807 | STL loss -0.2180 robustness\n",
            "step 0808 | STL loss -0.2180 robustness\n",
            "step 0809 | STL loss -0.2180 robustness\n",
            "step 0810 | STL loss -0.2180 robustness\n",
            "step 0811 | STL loss -0.2180 robustness\n",
            "step 0812 | STL loss -0.2180 robustness\n",
            "step 0813 | STL loss -0.2180 robustness\n",
            "step 0814 | STL loss -0.2180 robustness\n",
            "step 0815 | STL loss -0.2180 robustness\n",
            "step 0816 | STL loss -0.2180 robustness\n",
            "step 0817 | STL loss -0.2180 robustness\n",
            "step 0818 | STL loss -0.2180 robustness\n",
            "step 0819 | STL loss -0.2180 robustness\n",
            "step 0820 | STL loss -0.2180 robustness\n",
            "step 0821 | STL loss -0.2180 robustness\n",
            "step 0822 | STL loss -0.2180 robustness\n",
            "step 0823 | STL loss -0.2180 robustness\n",
            "step 0824 | STL loss -0.2180 robustness\n",
            "step 0825 | STL loss -0.2180 robustness\n",
            "step 0826 | STL loss -0.2180 robustness\n",
            "step 0827 | STL loss -0.2180 robustness\n",
            "step 0828 | STL loss -0.2180 robustness\n",
            "step 0829 | STL loss -0.2180 robustness\n",
            "step 0830 | STL loss -0.2180 robustness\n",
            "step 0831 | STL loss -0.2180 robustness\n",
            "step 0832 | STL loss -0.2180 robustness\n",
            "step 0833 | STL loss -0.2180 robustness\n",
            "step 0834 | STL loss -0.2180 robustness\n",
            "step 0835 | STL loss -0.2180 robustness\n",
            "step 0836 | STL loss -0.2180 robustness\n",
            "step 0837 | STL loss -0.2180 robustness\n",
            "step 0838 | STL loss -0.2180 robustness\n",
            "step 0839 | STL loss -0.2180 robustness\n",
            "step 0840 | STL loss -0.2180 robustness\n",
            "step 0841 | STL loss -0.2180 robustness\n",
            "step 0842 | STL loss -0.2180 robustness\n",
            "step 0843 | STL loss -0.2180 robustness\n",
            "step 0844 | STL loss -0.2180 robustness\n",
            "step 0845 | STL loss -0.2180 robustness\n",
            "step 0846 | STL loss -0.2180 robustness\n",
            "step 0847 | STL loss -0.2180 robustness\n",
            "step 0848 | STL loss -0.2180 robustness\n",
            "step 0849 | STL loss -0.2180 robustness\n",
            "step 0850 | STL loss -0.2180 robustness\n",
            "step 0851 | STL loss -0.2180 robustness\n",
            "step 0852 | STL loss -0.2180 robustness\n",
            "step 0853 | STL loss -0.2180 robustness\n",
            "step 0854 | STL loss -0.2180 robustness\n",
            "step 0855 | STL loss -0.2180 robustness\n",
            "step 0856 | STL loss -0.2180 robustness\n",
            "step 0857 | STL loss -0.2180 robustness\n",
            "step 0858 | STL loss -0.2180 robustness\n",
            "step 0859 | STL loss -0.2180 robustness\n",
            "step 0860 | STL loss -0.2180 robustness\n",
            "step 0861 | STL loss -0.2180 robustness\n",
            "step 0862 | STL loss -0.2180 robustness\n",
            "step 0863 | STL loss -0.2180 robustness\n",
            "step 0864 | STL loss -0.2180 robustness\n",
            "step 0865 | STL loss -0.2180 robustness\n",
            "step 0866 | STL loss -0.2180 robustness\n",
            "step 0867 | STL loss -0.2180 robustness\n",
            "step 0868 | STL loss -0.2180 robustness\n",
            "step 0869 | STL loss -0.2180 robustness\n",
            "step 0870 | STL loss -0.2180 robustness\n",
            "step 0871 | STL loss -0.2180 robustness\n",
            "step 0872 | STL loss -0.2180 robustness\n",
            "step 0873 | STL loss -0.2180 robustness\n",
            "step 0874 | STL loss -0.2180 robustness\n",
            "step 0875 | STL loss -0.2180 robustness\n",
            "step 0876 | STL loss -0.2180 robustness\n",
            "step 0877 | STL loss -0.2180 robustness\n",
            "step 0878 | STL loss -0.2180 robustness\n",
            "step 0879 | STL loss -0.2180 robustness\n",
            "step 0880 | STL loss -0.2180 robustness\n",
            "step 0881 | STL loss -0.2180 robustness\n",
            "step 0882 | STL loss -0.2180 robustness\n",
            "step 0883 | STL loss -0.2180 robustness\n",
            "step 0884 | STL loss -0.2180 robustness\n",
            "step 0885 | STL loss -0.2180 robustness\n",
            "step 0886 | STL loss -0.2180 robustness\n",
            "step 0887 | STL loss -0.2180 robustness\n",
            "step 0888 | STL loss -0.2180 robustness\n",
            "step 0889 | STL loss -0.2180 robustness\n",
            "step 0890 | STL loss -0.2180 robustness\n",
            "step 0891 | STL loss -0.2180 robustness\n",
            "step 0892 | STL loss -0.2180 robustness\n",
            "step 0893 | STL loss -0.2180 robustness\n",
            "step 0894 | STL loss -0.2180 robustness\n",
            "step 0895 | STL loss -0.2180 robustness\n",
            "step 0896 | STL loss -0.2180 robustness\n",
            "step 0897 | STL loss -0.2180 robustness\n",
            "step 0898 | STL loss -0.2180 robustness\n",
            "step 0899 | STL loss -0.2180 robustness\n",
            "step 0900 | STL loss -0.2180 robustness\n",
            "step 0901 | STL loss -0.2180 robustness\n",
            "step 0902 | STL loss -0.2180 robustness\n",
            "step 0903 | STL loss -0.2180 robustness\n",
            "step 0904 | STL loss -0.2180 robustness\n",
            "step 0905 | STL loss -0.2180 robustness\n",
            "step 0906 | STL loss -0.2180 robustness\n",
            "step 0907 | STL loss -0.2180 robustness\n",
            "step 0908 | STL loss -0.2180 robustness\n",
            "step 0909 | STL loss -0.2180 robustness\n",
            "step 0910 | STL loss -0.2180 robustness\n",
            "step 0911 | STL loss -0.2180 robustness\n",
            "step 0912 | STL loss -0.2180 robustness\n",
            "step 0913 | STL loss -0.2180 robustness\n",
            "step 0914 | STL loss -0.2180 robustness\n",
            "step 0915 | STL loss -0.2180 robustness\n",
            "step 0916 | STL loss -0.2180 robustness\n",
            "step 0917 | STL loss -0.2180 robustness\n",
            "step 0918 | STL loss -0.2180 robustness\n",
            "step 0919 | STL loss -0.2180 robustness\n",
            "step 0920 | STL loss -0.2180 robustness\n",
            "step 0921 | STL loss -0.2180 robustness\n",
            "step 0922 | STL loss -0.2180 robustness\n",
            "step 0923 | STL loss -0.2180 robustness\n",
            "step 0924 | STL loss -0.2180 robustness\n",
            "step 0925 | STL loss -0.2180 robustness\n",
            "step 0926 | STL loss -0.2180 robustness\n",
            "step 0927 | STL loss -0.2180 robustness\n",
            "step 0928 | STL loss -0.2180 robustness\n",
            "step 0929 | STL loss -0.2180 robustness\n",
            "step 0930 | STL loss -0.2180 robustness\n",
            "step 0931 | STL loss -0.2180 robustness\n",
            "step 0932 | STL loss -0.2180 robustness\n",
            "step 0933 | STL loss -0.2180 robustness\n",
            "step 0934 | STL loss -0.2180 robustness\n",
            "step 0935 | STL loss -0.2180 robustness\n",
            "step 0936 | STL loss -0.2180 robustness\n",
            "step 0937 | STL loss -0.2180 robustness\n",
            "step 0938 | STL loss -0.2180 robustness\n",
            "step 0939 | STL loss -0.2180 robustness\n",
            "step 0940 | STL loss -0.2180 robustness\n",
            "step 0941 | STL loss -0.2180 robustness\n",
            "step 0942 | STL loss -0.2180 robustness\n",
            "step 0943 | STL loss -0.2180 robustness\n",
            "step 0944 | STL loss -0.2180 robustness\n",
            "step 0945 | STL loss -0.2180 robustness\n",
            "step 0946 | STL loss -0.2180 robustness\n",
            "step 0947 | STL loss -0.2180 robustness\n",
            "step 0948 | STL loss -0.2180 robustness\n",
            "step 0949 | STL loss -0.2180 robustness\n",
            "step 0950 | STL loss -0.2180 robustness\n",
            "step 0951 | STL loss -0.2180 robustness\n",
            "step 0952 | STL loss -0.2180 robustness\n",
            "step 0953 | STL loss -0.2180 robustness\n",
            "step 0954 | STL loss -0.2180 robustness\n",
            "step 0955 | STL loss -0.2180 robustness\n",
            "step 0956 | STL loss -0.2180 robustness\n",
            "step 0957 | STL loss -0.2180 robustness\n",
            "step 0958 | STL loss -0.2180 robustness\n",
            "step 0959 | STL loss -0.2180 robustness\n",
            "step 0960 | STL loss -0.2180 robustness\n",
            "step 0961 | STL loss -0.2180 robustness\n",
            "step 0962 | STL loss -0.2180 robustness\n",
            "step 0963 | STL loss -0.2180 robustness\n",
            "step 0964 | STL loss -0.2180 robustness\n",
            "step 0965 | STL loss -0.2180 robustness\n",
            "step 0966 | STL loss -0.2180 robustness\n",
            "step 0967 | STL loss -0.2180 robustness\n",
            "step 0968 | STL loss -0.2180 robustness\n",
            "step 0969 | STL loss -0.2180 robustness\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "num_steps = 2000\n",
        "for step in range(1, num_steps + 1):\n",
        "    policy_params, opt_state, train_loss = update(policy_params, opt_state)\n",
        "\n",
        "    print(f\"step {step:04d} | STL loss {-train_loss:.4f} robustness\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}